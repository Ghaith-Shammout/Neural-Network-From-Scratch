{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network From Scratch\n",
    "\n",
    "## Introduction \n",
    "\n",
    "A neural network is a computational **model** inspired by the structure and function of the human brain. It consists of **layers** of interconnected units called **neurons** or **nodes**, which process input data by performing weighted computations and applying activation functions. Neural networks are particularly effective at learning patterns and representations from data, making them suitable for tasks like classification, regression, pattern recognition, and decision-making.\n",
    "\n",
    "### Goal of this project\n",
    "The goal of this project is to have deeper understanding of neural networks by diving deeper into their fundemental compoenents like activation functions, loss functions and many more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron Concept\n",
    "A **neuron** in a neural network is a computational unit that receives input signals from neurons in the previous layer, processes them by computing a weighted sum, add a bias, then applies an activation function, and passes the result to the next layer (in fully connected networks). \n",
    "\n",
    "- Neurons in the **input layer** don't process data—they simply hold the raw input features like pixel values or numerical data\n",
    "- Neurons in the **output layer** produce the final prediction or decision based on the computations from previous layers.  \n",
    "\n",
    " **Weights** ($w$) are numerical values associated with the connections between neurons in adjacent layers. They determine how much influence a particular input has on a neuron’s output. For every connection from an input to a neuron, there is a corresponding weight.\n",
    " \n",
    "**Bias** ($b$) is an additional parameter for each neuron that allows the model to shift the activation function, helping the network learn more flexible decision boundaries.\n",
    "\n",
    "The output of a neuron (before activation) is computed as:\n",
    "\n",
    "$$\n",
    "z = \\sum_{i=1}^{n} x_i \\cdot w_i + b\n",
    "$$\n",
    "\n",
    "where:\n",
    "* $x_i$ are the inputs,\n",
    "* $w_i$ are the corresponding weights,\n",
    "* $b$ is the bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.519999999999996\n"
     ]
    }
   ],
   "source": [
    "# This one illustrate three neurons' output to one neuron\n",
    "inputs = [1.2, 2.3, 3.1] # 3 neurons' output\n",
    "weights = [3.1, 2.1, 8.7] # 3 weights\n",
    "bias = 3 # 1 bias\n",
    "\n",
    "# The output of a neuron is computed as:\n",
    "output = inputs[0] * weights[0] + inputs[1] * weights[1] + inputs[2] * weights[2] + bias\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 2.4370000000000003, 0.17199999999999993]\n"
     ]
    }
   ],
   "source": [
    "# To illustrate four neurons to three neurons, we should have mulitpe wights with multiple basies\n",
    "input_layer = [1.2, 2.3, 3.1, 4.2]\n",
    "weights1 = [0.9, 0.1, -0.1, -0.3]\n",
    "weights2 = [0.5, -0.91, 0.3, 0.2]\n",
    "weights3 = [-0.1, 0.26, -0.26, -0.29]\n",
    "bias1 = 2\n",
    "bias2 = 3\n",
    "bias3 = 0.5\n",
    "\n",
    "layer1_neurons = [input_layer[0] * weights1[0] + input_layer[1] * weights1[1] + input_layer[2] * weights1[2] + bias1, #layer1_neuron1\n",
    "                  input_layer[0] * weights2[0] + input_layer[1] * weights2[1] + input_layer[2] * weights2[2] + bias2, #layer1_neuron2\n",
    "                  input_layer[0] * weights3[0] + input_layer[1] * weights3[1] + input_layer[2] * weights3[2] + bias3       #layer1_neuron3\n",
    "                ]\n",
    "\n",
    "print(layer1_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.74, 3.277, -1.046]\n"
     ]
    }
   ],
   "source": [
    "# Similar version using loops\n",
    "input_layer = [1.2, 2.3, 3.1, 4.2]\n",
    "\n",
    "weights = [[0.9, 0.1, -0.1, -0.3],\n",
    "           [0.5, -0.91, 0.3, 0.2],\n",
    "           [-0.1, 0.26, -0.26, -0.29]]\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "layer_outputs = []  #output of the current layer\n",
    "for neuron_weights, neuron_biases in zip(weights, biases):\n",
    "    neuron_output = 0 #output of the given neuron\n",
    "    for n_input, weight in zip(input_layer, neuron_weights):\n",
    "        neuron_output += n_input * weight\n",
    "    neuron_output += neuron_biases\n",
    "    layer_outputs.append(neuron_output)\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Math\n",
    "\n",
    "\n",
    "### What Is a \"Shape\" in Programming and Neural Networks?\n",
    "\n",
    "In the context of **machine learning, neural networks (NNs), and arrays (like NumPy or tensors in PyTorch/TensorFlow)**, the **\"shape\"** refers to the **dimensions** of a data structure—typically lists, arrays, or tensors.\n",
    "\n",
    "➤ Example: Python list\n",
    "\n",
    "```python\n",
    "a = [1, 2, 3]\n",
    "```\n",
    "\n",
    "* Shape: `(3,)` → a 1D list with 3 elements.\n",
    "\n",
    "➤ Example: 2D list (matrix)\n",
    "\n",
    "```python\n",
    "b = [[1, 2, 3],\n",
    "     [4, 5, 6]]\n",
    "```\n",
    "\n",
    "* Shape: `(2, 3)` → 2 rows, 3 columns.\n",
    "\n",
    "Think of shape like the size blueprint of the data structure.\n",
    "\n",
    "---\n",
    "\n",
    "### How Can We Identify the Shape of a List?\n",
    "\n",
    "In Python, you can use:\n",
    "\n",
    "#### NumPy \n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "arr = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(arr.shape)  # Output: (2, 3)\n",
    "```\n",
    "\n",
    "####  For native Python lists:\n",
    "\n",
    "You can inspect shape manually:\n",
    "\n",
    "```python\n",
    "lst = [[1, 2, 3], [4, 5, 6]]\n",
    "rows = len(lst)\n",
    "cols = len(lst[0])\n",
    "print((rows, cols))  # (2, 3)\n",
    "```\n",
    "\n",
    "But it's less robust since NumPy handles it more safely.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Is Shape Important in Neural Networks?\n",
    "\n",
    "#### 1. **Shape dictates compatibility between layers**\n",
    "\n",
    "Each layer in a neural network expects input and produces output of certain shapes.\n",
    "\n",
    "Example:\n",
    "\n",
    "* If a layer expects input shape `(batch_size, 3)` and you pass `(batch_size, 2)`, it will **crash**.\n",
    "* Matrix multiplication (used in dense layers) **requires shape compatibility**:\n",
    "\n",
    "  $$\n",
    "  (m \\times n) \\cdot (n \\times p) \\rightarrow (m \\times p)\n",
    "  $$\n",
    "\n",
    "#### 2. **Bugs from shape mismatches are common**\n",
    "\n",
    "Incorrect shapes cause:\n",
    "\n",
    "* Runtime errors (`ValueError`, `shape mismatch`)\n",
    "* Incorrect learning (e.g., if dimensions get unintentionally broadcasted)\n",
    "\n",
    "#### 3. **Shapes guide how to reshape, flatten, or batch data**\n",
    "\n",
    "Before feeding data into an NN:\n",
    "\n",
    "* Images often need shape `(batch, height, width, channels)`\n",
    "* Flattening or reshaping is often needed for Dense layers\n",
    "\n",
    "#### 4. **Backpropagation depends on gradients with matching shapes**\n",
    "\n",
    "All derivatives computed during training **must match shape** with their corresponding weights and activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain Dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8\n"
     ]
    }
   ],
   "source": [
    "# Dot Product using Numpy\n",
    "import numpy as np\n",
    "\n",
    "inputs = [1, 2, 3, 2.5]\n",
    "weights = [0.2, 0.8, -0.5, 1.0]\n",
    "bias = 2\n",
    "\n",
    "# We should know that the order in the dot operation matters, not in this example but,\n",
    "# in the example where we have multiple weights (multiple neurons in the next layer), see example below\n",
    "output = np.dot(inputs, weights) + bias\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batches, Layers & Objects\n",
    "\n",
    "### Batches - Number of training samples at once.\n",
    "In the context of neural networks, a batch refers to a subset of the training data that is processed at once before updating the model's parameters. The batch size is the number of training samples within that subset. Instead of processing the entire training dataset at once (which is called batch gradient descent), the data is divided into smaller batches to make training more efficient.\n",
    "\n",
    "Multiple samples from input will be multipled with the same weight vectors and added to the same bias. We should not care to much about these values as we will see that weights' values and baiases will be adjusted using forward and backword propagation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.8    1.21   2.385]\n",
      " [ 8.9   -1.81   0.2  ]\n",
      " [ 1.41   1.051  0.026]]\n"
     ]
    }
   ],
   "source": [
    "sing_samp_input = [1, -6, 4, 9] # These are features from a single sample\n",
    "mult_samp_input = [[1, 2, 3, 2.5],  # These are features from multiple samples\n",
    "                   [2.0, 5.0, -1.0, 2.0],\n",
    "                   [-1.5, 2.7, 3.3, -0.8]\n",
    "                  ]\n",
    "\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],\n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]]\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "output = np.dot(mult_samp_input, np.array(weights).T) + biases\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5031  -1.04185 -2.03875]\n",
      " [ 0.2434  -2.7332  -5.7633 ]\n",
      " [-0.99314  1.41254 -0.35655]]\n"
     ]
    }
   ],
   "source": [
    "# Now we will create multiple layers with batch size > 1\n",
    "mult_samp_input = [[1, 2, 3, 2.5],\n",
    "                   [2.0, 5.0, -1.0, 2.0],\n",
    "                   [-1.5, 2.7, 3.3, -0.8]\n",
    "                  ]\n",
    "\n",
    "l1_weights = [[0.2, 0.8, -0.5, 1.0],\n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]]\n",
    "l1_biases = [2, 3, 0.5]\n",
    "\n",
    "l2_weights = [[0.1, -0.14, 0.5],\n",
    "              [-0.5, 0.12, -0.33],\n",
    "              [-0.44, 0.73, -0.13]]\n",
    "l2_biases = [-1, 2, -0.5]\n",
    "\n",
    "l1_output = np.dot(mult_samp_input, np.array(l1_weights).T) + l1_biases\n",
    "l2_output = np.dot(l1_output, np.array(l2_weights).T) + l2_biases\n",
    "print(l2_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.10758131  1.03983522  0.24462411  0.31821498  0.18851053]\n",
      " [-0.08349796  0.70846411  0.00293357  0.44701525  0.36360538]\n",
      " [-0.50763245  0.55688422  0.07987797 -0.34889573  0.04553042]]\n",
      "[[ 0.148296   -0.08397602]\n",
      " [ 0.14100315 -0.01340469]\n",
      " [ 0.20124979 -0.07290616]]\n",
      "[[-0.00087785]\n",
      " [ 0.00167789]\n",
      " [ 0.00036128]]\n"
     ]
    }
   ],
   "source": [
    "# Now, we will convert this into an object\n",
    "np.random.seed(0)\n",
    "\n",
    "X =  [[1, 2, 3, 2.5],\n",
    "      [2.0, 5.0, -1.0, 2.0],\n",
    "      [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons) # initialize weights\n",
    "        self.biases = np.zeros((1, n_neurons)) # initialize biases\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases # compute the output\n",
    "\n",
    "layer1 = Layer_Dense(4, 5)\n",
    "layer2 = Layer_Dense(5, 2)\n",
    "layer3 = Layer_Dense(2, 1)\n",
    "\n",
    "layer1.forward(X)\n",
    "print(layer1.output)\n",
    "layer2.forward(layer1.output)\n",
    "print(layer2.output)\n",
    "layer3.forward(layer2.output)\n",
    "print(layer3.output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.        ]\n",
      " [-0.00104752  0.00113954 -0.00047984]\n",
      " [-0.00274148  0.00317292 -0.00086922]\n",
      " [-0.00421884  0.00526663 -0.00055913]\n",
      " [-0.00577077  0.00714014 -0.0008943 ]]\n"
     ]
    }
   ],
   "source": [
    "import nnfs \n",
    "from nnfs.datasets import spiral_data\n",
    "# initialize the nnfs\n",
    "nnfs.init()\n",
    "\n",
    "# create dataset    \n",
    "X, y = spiral_data(100, 3)\n",
    "\n",
    "# create dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "# perform a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "print(dense1.output[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Function:\n",
    "    def ReLU(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "    \n",
    "    def dReLU(self, dvalues):\n",
    "        self.output = np.where(dvalues > 0, 1, 0)\n",
    "    \n",
    "    def sigmoid(self, inputs):\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "    \n",
    "    def dsigmoid(self, dvalues):\n",
    "        self.output = dvalues * (1 - dvalues)\n",
    "    \n",
    "    def tanh(self, inputs):\n",
    "        self.output = np.tanh(inputs)\n",
    "    \n",
    "    def dtanh(self, dvalues):\n",
    "        self.output = 1 - np.tanh(dvalues) ** 2\n",
    "    \n",
    "    def softmax(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        self.output = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]]\n"
     ]
    }
   ],
   "source": [
    "# create dataset\n",
    "X, y = spiral_data(100, 3)\n",
    "# create dense layer (wieghts & biases) with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "# create activation function\n",
    "activation1 = Activation_Function()\n",
    "# create second dense layer with 3 input features and 3 output values\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "# create activation function\n",
    "# activation2 = Activation_Function()\n",
    "\n",
    "\n",
    "# perform a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "activation1.ReLU(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "activation1.softmax(dense2.output)\n",
    "\n",
    "print(activation1.output[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "### What Is a Loss Function?\n",
    "\n",
    "A **loss function** is a **mathematical function** that measures how **far off a model's prediction is from the actual target value**.\n",
    "\n",
    "> Think of it like a penalty score: the higher the loss, the worse your model is doing.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Do We Use a Loss Function?\n",
    "\n",
    "We use loss functions to:\n",
    "\n",
    "1. **Quantify model error** — You can't improve what you can't measure.\n",
    "2. **Guide learning** — During training, optimization algorithms (like gradient descent) use the **loss value to adjust weights**.\n",
    "3. **Compare models** — Lower loss typically indicates a better-performing model (for the same problem).\n",
    "\n",
    "---\n",
    "\n",
    "###  When and How Do We Use a Loss Function?\n",
    "\n",
    "➤ **When**\n",
    "\n",
    "* Every time the model makes a prediction during training.\n",
    "* In **each iteration**, the loss function is used to evaluate the model’s output against ground truth.\n",
    "\n",
    "➤ **How**\n",
    "\n",
    "1. **Prediction:** The model outputs a value (e.g., 0.8 for class \"dog\")\n",
    "2. **Compare with target:** The ground truth is 1 (it *is* a dog)\n",
    "3. **Loss function:** Calculates how \"wrong\" 0.8 is compared to 1 (e.g., using MSE or Cross-Entropy)\n",
    "4. **Backpropagation:** Uses the loss to compute gradients and update weights\n",
    "\n",
    "---\n",
    "\n",
    "### Common Types of Loss Functions\n",
    "\n",
    "Loss functions vary depending on the type of task: **regression** vs **classification**.\n",
    "\n",
    "---\n",
    "\n",
    "####  1. **For Regression Tasks**\n",
    "\n",
    "Where the model predicts continuous values.\n",
    "\n",
    "**Mean Squared Error (MSE)**\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "* Penalizes large errors more than small ones\n",
    "* Very common in predicting numbers (e.g., house prices)\n",
    "\n",
    "**Mean Absolute Error (MAE)**\n",
    "\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "* Treats all errors equally, less sensitive to outliers\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **For Classification Tasks**\n",
    "\n",
    "Where the model predicts categories or probabilities.\n",
    "\n",
    "**Binary Cross-Entropy**\n",
    "\n",
    "$$\n",
    "\\text{BCE} = -[y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y})]\n",
    "$$\n",
    "\n",
    "* Used for binary classification (`y = 0` or `1`)\n",
    "* Output should be a probability (`sigmoid` activation)\n",
    "\n",
    "**Categorical Cross-Entropy**\n",
    "\n",
    "$$\n",
    "\\text{CCE} = -\\sum_{i} y_i \\cdot \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "* Used for multi-class classification\n",
    "* Targets are one-hot encoded, predictions are softmax probabilities\n",
    "\n",
    "**Sparse Categorical Cross-Entropy**\n",
    "\n",
    "* Same as above, but labels are integers instead of one-hot\n",
    "* Easier to use with many classes\n",
    "\n",
    "---\n",
    "\n",
    "#### Bonus: Other Losses\n",
    "\n",
    "* **Huber Loss** – robust to outliers (mix between MSE and MAE)\n",
    "* **KL Divergence** – measures difference between two probability distributions\n",
    "* **Contrastive Loss / Triplet Loss** – used in face verification, embeddings\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Type           | Task                       | Function                         | Example Use                | \n",
    "| -------------- | -------------------------- | -------------------------------- | -------------------------- | \n",
    "| MSE            | Regression                 | $\\frac{1}{n}\\sum(y - \\hat{y})^2$ | Predicting temperatures    |\n",
    "| MAE            | Regression                 | $\\frac{1}{n}\\sum(y - \\hat{y})$   | Forecasting sales          |\n",
    "| Binary CE      | Binary Classification      | BCE formula above                | Spam detection             |\n",
    "| Categorical CE | Multi-Class Classification | CCE formula                      | Digit classification (0–9) |\n",
    "| Sparse CE      | Multi-Class (int labels)   | Same as CCE                      | NLP classification tasks   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
