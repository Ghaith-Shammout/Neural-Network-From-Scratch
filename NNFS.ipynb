{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network From Scratch\n",
    "\n",
    "## Introduction \n",
    "\n",
    "A neural network is a computational **model** inspired by the structure and function of the human brain. It consists of **layers** of interconnected units called **neurons** or **nodes**, which process input data by performing weighted computations and applying activation functions. Neural networks are particularly effective at learning patterns and representations from data, making them suitable for tasks like classification, regression, pattern recognition, and decision-making.\n",
    "\n",
    "### Goal of this project\n",
    "The goal of this project is to have deeper understanding of neural networks by diving deeper into their fundemental compoenents like activation functions, loss functions and many more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron Concept\n",
    "A **neuron** in a neural network is a computational unit that receives input signals from neurons in the previous layer, processes them by computing a weighted sum, applies an activation function, and passes the result to the next layer (in fully connected networks). Neurons in the **input layer** don't process data—they simply hold the raw input features like pixel values or numerical data. Neurons in the **output layer** produce the final prediction or decision based on the computations from previous layers.  \n",
    "\n",
    " **Weights** are numerical values associated with the connections between neurons in adjacent layers. They determine how much influence a particular input has on a neuron’s output. For every connection from an input to a neuron, there is a corresponding weight.\n",
    " \n",
    "**Bias** is an additional parameter for each neuron that allows the model to shift the activation function, helping the network learn more flexible decision boundaries.\n",
    "\n",
    "The output of a neuron (before activation) is computed as:\n",
    "\n",
    "$$\n",
    "z = \\sum_{i=1}^{n} x_i \\cdot w_i + b\n",
    "$$\n",
    "\n",
    "where:\n",
    "* $x_i$ are the inputs,\n",
    "* $w_i$ are the corresponding weights,\n",
    "* $b$ is the bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.519999999999996\n"
     ]
    }
   ],
   "source": [
    "# This one illustrate three neurons to one neuron\n",
    "input_layer = [1.2, 2.3, 3.1]\n",
    "weights = [3.1, 2.1, 8.7]\n",
    "bias = 3\n",
    "\n",
    "neuron = input_layer[0] * weights[0] + input_layer[1] * weights[1] + input_layer[2] * weights[2] + bias\n",
    "print(neuron)\n",
    "#output = np.dot(inputs, weights) + bias\n",
    "#print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 2.4370000000000003, 0.17199999999999993]\n"
     ]
    }
   ],
   "source": [
    "# To illustrate four neurons to three neurons, we should have mulitpe wights with multiple basies\n",
    "input_layer = [1.2, 2.3, 3.1, 4.2]\n",
    "weights1 = [0.9, 0.1, -0.1, -0.3]\n",
    "weights2 = [0.5, -0.91, 0.3, 0.2]\n",
    "weights3 = [-0.1, 0.26, -0.26, -0.29]\n",
    "bias1 = 2\n",
    "bias2 = 3\n",
    "bias3 = 0.5\n",
    "\n",
    "layer1_neurons = [input_layer[0] * weights1[0] + input_layer[1] * weights1[1] + input_layer[2] * weights1[2] + bias1, #layer1_neuron1\n",
    "                  input_layer[0] * weights2[0] + input_layer[1] * weights2[1] + input_layer[2] * weights2[2] + bias2, #layer1_neuron2\n",
    "                  input_layer[0] * weights3[0] + input_layer[1] * weights3[1] + input_layer[2] * weights3[2] + bias3       #layer1_neuron3\n",
    "                ]\n",
    "\n",
    "print(layer1_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.74, 3.277, -1.046]\n"
     ]
    }
   ],
   "source": [
    "# Similar version using loops\n",
    "input_layer = [1.2, 2.3, 3.1, 4.2]\n",
    "\n",
    "weights = [[0.9, 0.1, -0.1, -0.3],\n",
    "           [0.5, -0.91, 0.3, 0.2],\n",
    "           [-0.1, 0.26, -0.26, -0.29]]\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "layer_outputs = []  #output of the current layer\n",
    "for neuron_weights, neuron_biases in zip(weights, biases):\n",
    "    neuron_output = 0 #output of the given neuron\n",
    "    for n_input, weight in zip(input_layer, neuron_weights):\n",
    "        neuron_output += n_input * weight\n",
    "    neuron_output += neuron_biases\n",
    "    layer_outputs.append(neuron_output)\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Math\n",
    "**Shape**\n",
    "Explain what Shape is\n",
    "\n",
    "Explain **dot product**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Dot Product using Numpy\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m inputs = [\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m2.5\u001b[39m]\n\u001b[32m      5\u001b[39m weights = [\u001b[32m0.2\u001b[39m, \u001b[32m0.8\u001b[39m, -\u001b[32m0.5\u001b[39m, \u001b[32m1.0\u001b[39m]\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# Dot Product using Numpy\n",
    "import numpy as np\n",
    "\n",
    "inputs = [1, 2, 3, 2.5]\n",
    "weights = [0.2, 0.8, -0.5, 1.0]\n",
    "bias = 2\n",
    "\n",
    "# We should know that the order in the dot operation matters, not in this example but,\n",
    "# in the example where we have multiple weights (multiple neurons in the next layer), see example below\n",
    "output = np.dot(inputs, weights) + bias\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
